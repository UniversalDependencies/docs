---
layout: base
title:  'Tokenization'
---


# Tokenization and Word Boundaries

#### Demonstrative and Emphatic Suffixes
* Frequently occurring affixes such as demonstrative and emphatic suffixes are always separated from preceding tokens and considered to be tokens themselves:
    * _cuci sium_

#### Conjoined Tokens
* Where forms of a significant part of speech such as the **article**, the **copula**, or **possessive pronouns** occur in reduced or altered form when combined with other tokens, these forms are considered to be conjoined tokens:
    * _cosin, iarsint, isnaib, frisna, ón, isin_
    * _manam = mo anam, m'athair = mo athair_

#### Compounds
* separate or mwe?
    * _drochcostud_

#### Verbal Complex
* preverbs and infixed pronouns considered part of the same token and kept together:
    * _dagníu_ = do-a-gníu

# Normalizing Orthography

#### LENITION, BRACKETS, EDITOR-SPECIFIC NORMALIZATION

## References
* [A Character-Level LSTM Network Model for Tokenizing the Old Irish text of the Würzburg Glosses on the Pauline Epistles](https://pdfs.semanticscholar.org/ca39/e7b169034e048e4a03eb73588dd02cc1fb61.pdf?_ga=2.226142810.1785128679.1589418175-1340948691.1589214695)
